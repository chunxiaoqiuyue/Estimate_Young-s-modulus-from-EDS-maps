{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from PIL import Image,ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.preprocessing.image import load_img,img_to_array,array_to_img,ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Sequential,Model, load_model\n",
    "\n",
    "from keras.layers import Input, Activation, Flatten, Dense, Flatten,Conv2D, Dropout, MaxPool2D, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "from keras.callbacks.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EDS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X_Dataset2.npy')\n",
    "Y = np.load('Y_Dataset2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_EDS_train,X_EDS_test,Y_EDS_train,Y_EDS_test=train_test_split(X,Y,test_size=0.25,random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_mod_3(lr = 0.00001):\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(112,112,9),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    model.add(Dense(units=100,activation=\"relu\"))\n",
    "    model.add(Dense(units=100,activation=\"relu\"))\n",
    "    model.add(Dense(units=100,activation=\"relu\"))\n",
    "    model.add(Dense(units=1, activation=\"linear\"))\n",
    "    \n",
    "    \n",
    "    sgd = optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(optimizer=sgd,loss=\"mean_absolute_percentage_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_7 = vgg16_mod_3(lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_eds_10312021_lr_7.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/300\n",
      "600/600 [==============================] - 57s 94ms/step - loss: 100.4301 - val_loss: 100.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 100.02737, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 2/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 99.6555 - val_loss: 99.2949\n",
      "\n",
      "Epoch 00002: val_loss improved from 100.02737 to 99.29492, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 3/300\n",
      "600/600 [==============================] - 56s 93ms/step - loss: 98.9702 - val_loss: 98.6784\n",
      "\n",
      "Epoch 00003: val_loss improved from 99.29492 to 98.67837, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 4/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 98.4052 - val_loss: 98.2039\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.67837 to 98.20388, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 5/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 97.9757 - val_loss: 97.8299\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.20388 to 97.82987, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 6/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 97.6193 - val_loss: 97.4956\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.82987 to 97.49565, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 7/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 97.2951 - val_loss: 97.1848\n",
      "\n",
      "Epoch 00007: val_loss improved from 97.49565 to 97.18476, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 8/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 96.9853 - val_loss: 96.8810\n",
      "\n",
      "Epoch 00008: val_loss improved from 97.18476 to 96.88099, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 9/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 96.6729 - val_loss: 96.5695\n",
      "\n",
      "Epoch 00009: val_loss improved from 96.88099 to 96.56953, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 10/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 96.3499 - val_loss: 96.2441\n",
      "\n",
      "Epoch 00010: val_loss improved from 96.56953 to 96.24411, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 11/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 96.0116 - val_loss: 95.9023\n",
      "\n",
      "Epoch 00011: val_loss improved from 96.24411 to 95.90235, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 12/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 95.6520 - val_loss: 95.5357\n",
      "\n",
      "Epoch 00012: val_loss improved from 95.90235 to 95.53567, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 13/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 95.2639 - val_loss: 95.1361\n",
      "\n",
      "Epoch 00013: val_loss improved from 95.53567 to 95.13612, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 14/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 94.8390 - val_loss: 94.6964\n",
      "\n",
      "Epoch 00014: val_loss improved from 95.13612 to 94.69638, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 15/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 94.3671 - val_loss: 94.2022\n",
      "\n",
      "Epoch 00015: val_loss improved from 94.69638 to 94.20223, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 16/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 93.8339 - val_loss: 93.6387\n",
      "\n",
      "Epoch 00016: val_loss improved from 94.20223 to 93.63871, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 17/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 93.2255 - val_loss: 92.9971\n",
      "\n",
      "Epoch 00017: val_loss improved from 93.63871 to 92.99714, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 18/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 92.5350 - val_loss: 92.2748\n",
      "\n",
      "Epoch 00018: val_loss improved from 92.99714 to 92.27484, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 19/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 91.7558 - val_loss: 91.4595\n",
      "\n",
      "Epoch 00019: val_loss improved from 92.27484 to 91.45948, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 20/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 90.8724 - val_loss: 90.5294\n",
      "\n",
      "Epoch 00020: val_loss improved from 91.45948 to 90.52940, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 21/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 89.8604 - val_loss: 89.4560\n",
      "\n",
      "Epoch 00021: val_loss improved from 90.52940 to 89.45602, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 22/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 88.6865 - val_loss: 88.2058\n",
      "\n",
      "Epoch 00022: val_loss improved from 89.45602 to 88.20580, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 23/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 87.3056 - val_loss: 86.7214\n",
      "\n",
      "Epoch 00023: val_loss improved from 88.20580 to 86.72143, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 24/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 85.6537 - val_loss: 84.9234\n",
      "\n",
      "Epoch 00024: val_loss improved from 86.72143 to 84.92337, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 25/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 83.6389 - val_loss: 82.7168\n",
      "\n",
      "Epoch 00025: val_loss improved from 84.92337 to 82.71685, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 26/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 81.1423 - val_loss: 79.9549\n",
      "\n",
      "Epoch 00026: val_loss improved from 82.71685 to 79.95485, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 27/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 77.9677 - val_loss: 76.3835\n",
      "\n",
      "Epoch 00027: val_loss improved from 79.95485 to 76.38348, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 28/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 73.7540 - val_loss: 71.4811\n",
      "\n",
      "Epoch 00028: val_loss improved from 76.38348 to 71.48107, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 29/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 67.8395 - val_loss: 64.5212\n",
      "\n",
      "Epoch 00029: val_loss improved from 71.48107 to 64.52123, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 30/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 59.2468 - val_loss: 54.1101\n",
      "\n",
      "Epoch 00030: val_loss improved from 64.52123 to 54.11009, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 31/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 45.8312 - val_loss: 37.0730\n",
      "\n",
      "Epoch 00031: val_loss improved from 54.11009 to 37.07301, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 32/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 23.5526 - val_loss: 13.1366\n",
      "\n",
      "Epoch 00032: val_loss improved from 37.07301 to 13.13662, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 33/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 11.0323 - val_loss: 11.1509\n",
      "\n",
      "Epoch 00033: val_loss improved from 13.13662 to 11.15092, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 34/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 10.9010 - val_loss: 11.2729\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 11.15092\n",
      "Epoch 35/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 10.8634 - val_loss: 11.2707\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 11.15092\n",
      "Epoch 36/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 10.8262 - val_loss: 11.2236\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 11.15092\n",
      "Epoch 37/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.8009 - val_loss: 11.2211\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 11.15092\n",
      "Epoch 38/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.7726 - val_loss: 11.1683\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 11.15092\n",
      "Epoch 39/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.7453 - val_loss: 11.2342\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 11.15092\n",
      "Epoch 40/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.7584 - val_loss: 11.1634\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 11.15092\n",
      "Epoch 41/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 10.7253 - val_loss: 11.1395\n",
      "\n",
      "Epoch 00041: val_loss improved from 11.15092 to 11.13953, saving model to vgg16_eds_10312021_lr_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.6716 - val_loss: 11.2228\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 11.13953\n",
      "Epoch 43/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 10.6658 - val_loss: 11.0950\n",
      "\n",
      "Epoch 00043: val_loss improved from 11.13953 to 11.09502, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 44/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.6342 - val_loss: 11.0624\n",
      "\n",
      "Epoch 00044: val_loss improved from 11.09502 to 11.06243, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 45/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.6024 - val_loss: 11.0928\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 11.06243\n",
      "Epoch 46/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.6077 - val_loss: 10.9888\n",
      "\n",
      "Epoch 00046: val_loss improved from 11.06243 to 10.98881, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 47/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 10.5748 - val_loss: 11.0246\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 10.98881\n",
      "Epoch 48/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 10.5420 - val_loss: 10.9783\n",
      "\n",
      "Epoch 00048: val_loss improved from 10.98881 to 10.97833, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 49/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 10.5405 - val_loss: 11.0451\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 10.97833\n",
      "Epoch 50/300\n",
      "600/600 [==============================] - 56s 94ms/step - loss: 10.5277 - val_loss: 11.0614\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 10.97833\n",
      "Epoch 51/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.4773 - val_loss: 10.9564\n",
      "\n",
      "Epoch 00051: val_loss improved from 10.97833 to 10.95636, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 52/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.4618 - val_loss: 10.9909\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 10.95636\n",
      "Epoch 53/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 10.4736 - val_loss: 10.9881\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 10.95636\n",
      "Epoch 54/300\n",
      "600/600 [==============================] - 59s 99ms/step - loss: 10.4157 - val_loss: 10.9039\n",
      "\n",
      "Epoch 00054: val_loss improved from 10.95636 to 10.90395, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 55/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 10.3908 - val_loss: 10.9049\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 10.90395\n",
      "Epoch 56/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.3883 - val_loss: 10.8445\n",
      "\n",
      "Epoch 00056: val_loss improved from 10.90395 to 10.84452, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 57/300\n",
      "600/600 [==============================] - 58s 97ms/step - loss: 10.3511 - val_loss: 10.8629\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 10.84452\n",
      "Epoch 58/300\n",
      "600/600 [==============================] - 56s 94ms/step - loss: 10.3406 - val_loss: 10.7968\n",
      "\n",
      "Epoch 00058: val_loss improved from 10.84452 to 10.79683, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 59/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.3281 - val_loss: 10.8814\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 10.79683\n",
      "Epoch 60/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.3545 - val_loss: 10.7673\n",
      "\n",
      "Epoch 00060: val_loss improved from 10.79683 to 10.76730, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 61/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 10.2971 - val_loss: 10.7650\n",
      "\n",
      "Epoch 00061: val_loss improved from 10.76730 to 10.76501, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 62/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 10.2686 - val_loss: 10.7466\n",
      "\n",
      "Epoch 00062: val_loss improved from 10.76501 to 10.74661, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 63/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 10.2348 - val_loss: 10.9139\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 10.74661\n",
      "Epoch 64/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.2251 - val_loss: 10.7296\n",
      "\n",
      "Epoch 00064: val_loss improved from 10.74661 to 10.72958, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 65/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 10.1997 - val_loss: 10.7417\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 10.72958\n",
      "Epoch 66/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.1957 - val_loss: 10.7275\n",
      "\n",
      "Epoch 00066: val_loss improved from 10.72958 to 10.72747, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 67/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 10.1830 - val_loss: 10.7841\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 10.72747\n",
      "Epoch 68/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.1425 - val_loss: 10.7184\n",
      "\n",
      "Epoch 00068: val_loss improved from 10.72747 to 10.71838, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 69/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 10.1296 - val_loss: 10.6871\n",
      "\n",
      "Epoch 00069: val_loss improved from 10.71838 to 10.68705, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 70/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 10.1141 - val_loss: 10.6953\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 10.68705\n",
      "Epoch 71/300\n",
      "600/600 [==============================] - 95s 159ms/step - loss: 10.0855 - val_loss: 10.6517\n",
      "\n",
      "Epoch 00071: val_loss improved from 10.68705 to 10.65167, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 72/300\n",
      "600/600 [==============================] - 59s 98ms/step - loss: 10.0702 - val_loss: 10.6198\n",
      "\n",
      "Epoch 00072: val_loss improved from 10.65167 to 10.61976, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 73/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 10.0699 - val_loss: 10.6154\n",
      "\n",
      "Epoch 00073: val_loss improved from 10.61976 to 10.61545, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 74/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 10.0362 - val_loss: 10.6450\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 10.61545\n",
      "Epoch 75/300\n",
      "600/600 [==============================] - 56s 93ms/step - loss: 10.0108 - val_loss: 10.6010\n",
      "\n",
      "Epoch 00075: val_loss improved from 10.61545 to 10.60097, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 76/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.9884 - val_loss: 10.6431\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 10.60097\n",
      "Epoch 77/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 9.9700 - val_loss: 10.5515\n",
      "\n",
      "Epoch 00077: val_loss improved from 10.60097 to 10.55149, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 78/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 9.9784 - val_loss: 10.6610\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 10.55149\n",
      "Epoch 79/300\n",
      "600/600 [==============================] - 64s 107ms/step - loss: 9.9682 - val_loss: 10.5869\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 10.55149\n",
      "Epoch 80/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 9.9300 - val_loss: 10.5671\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 10.55149\n",
      "Epoch 81/300\n",
      "600/600 [==============================] - 59s 99ms/step - loss: 9.9278 - val_loss: 10.5983\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 10.55149\n",
      "Epoch 82/300\n",
      "600/600 [==============================] - 61s 101ms/step - loss: 9.8980 - val_loss: 10.5717\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 10.55149\n",
      "Epoch 83/300\n",
      "600/600 [==============================] - 56s 93ms/step - loss: 9.9025 - val_loss: 10.4922\n",
      "\n",
      "Epoch 00083: val_loss improved from 10.55149 to 10.49219, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 84/300\n",
      "600/600 [==============================] - 57s 95ms/step - loss: 9.8521 - val_loss: 10.5273\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 10.49219\n",
      "Epoch 85/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 9.8642 - val_loss: 10.5240\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 10.49219\n",
      "Epoch 86/300\n",
      "600/600 [==============================] - 64s 106ms/step - loss: 9.8318 - val_loss: 10.4651\n",
      "\n",
      "Epoch 00086: val_loss improved from 10.49219 to 10.46514, saving model to vgg16_eds_10312021_lr_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "600/600 [==============================] - 65s 108ms/step - loss: 9.8179 - val_loss: 10.4358\n",
      "\n",
      "Epoch 00087: val_loss improved from 10.46514 to 10.43577, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 88/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.7877 - val_loss: 10.4204\n",
      "\n",
      "Epoch 00088: val_loss improved from 10.43577 to 10.42045, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 89/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.8338 - val_loss: 10.4281\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 10.42045\n",
      "Epoch 90/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.7577 - val_loss: 10.4226\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 10.42045\n",
      "Epoch 91/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.7617 - val_loss: 10.3935\n",
      "\n",
      "Epoch 00091: val_loss improved from 10.42045 to 10.39350, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 92/300\n",
      "600/600 [==============================] - 56s 94ms/step - loss: 9.7326 - val_loss: 10.3996\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 10.39350\n",
      "Epoch 93/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.7298 - val_loss: 10.3734\n",
      "\n",
      "Epoch 00093: val_loss improved from 10.39350 to 10.37342, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 94/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.7018 - val_loss: 10.3212\n",
      "\n",
      "Epoch 00094: val_loss improved from 10.37342 to 10.32124, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 95/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.7217 - val_loss: 10.3371\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 10.32124\n",
      "Epoch 96/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.7015 - val_loss: 10.4482\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 10.32124\n",
      "Epoch 97/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6582 - val_loss: 10.3227\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 10.32124\n",
      "Epoch 98/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6473 - val_loss: 10.2799\n",
      "\n",
      "Epoch 00098: val_loss improved from 10.32124 to 10.27994, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 99/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6399 - val_loss: 10.2843\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 10.27994\n",
      "Epoch 100/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6328 - val_loss: 10.3553\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 10.27994\n",
      "Epoch 101/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.6489 - val_loss: 10.4089\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 10.27994\n",
      "Epoch 102/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6353 - val_loss: 10.2275\n",
      "\n",
      "Epoch 00102: val_loss improved from 10.27994 to 10.22748, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 103/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.6023 - val_loss: 10.2435\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 10.22748\n",
      "Epoch 104/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.5614 - val_loss: 10.2279\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 10.22748\n",
      "Epoch 105/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.5455 - val_loss: 10.2016\n",
      "\n",
      "Epoch 00105: val_loss improved from 10.22748 to 10.20165, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 106/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 9.5385 - val_loss: 10.1881\n",
      "\n",
      "Epoch 00106: val_loss improved from 10.20165 to 10.18815, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 107/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.5565 - val_loss: 10.2390\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 10.18815\n",
      "Epoch 108/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.5128 - val_loss: 10.2983\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 10.18815\n",
      "Epoch 109/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.5564 - val_loss: 10.2230\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 10.18815\n",
      "Epoch 110/300\n",
      "600/600 [==============================] - 56s 94ms/step - loss: 9.4958 - val_loss: 10.1394\n",
      "\n",
      "Epoch 00110: val_loss improved from 10.18815 to 10.13937, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 111/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.4855 - val_loss: 10.1297\n",
      "\n",
      "Epoch 00111: val_loss improved from 10.13937 to 10.12971, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 112/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.4636 - val_loss: 10.1537\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 10.12971\n",
      "Epoch 113/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.4448 - val_loss: 10.1308\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 10.12971\n",
      "Epoch 114/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.4454 - val_loss: 10.1253\n",
      "\n",
      "Epoch 00114: val_loss improved from 10.12971 to 10.12526, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 115/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.4212 - val_loss: 10.0595\n",
      "\n",
      "Epoch 00115: val_loss improved from 10.12526 to 10.05954, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 116/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.4222 - val_loss: 10.1354\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 10.05954\n",
      "Epoch 117/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.4469 - val_loss: 10.0891\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 10.05954\n",
      "Epoch 118/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.3897 - val_loss: 10.1196\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 10.05954\n",
      "Epoch 119/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.3863 - val_loss: 10.0608\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 10.05954\n",
      "Epoch 120/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.3798 - val_loss: 10.0123\n",
      "\n",
      "Epoch 00120: val_loss improved from 10.05954 to 10.01230, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 121/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.4384 - val_loss: 10.0596\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 10.01230\n",
      "Epoch 122/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 9.3426 - val_loss: 10.0086\n",
      "\n",
      "Epoch 00122: val_loss improved from 10.01230 to 10.00863, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 123/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 9.4056 - val_loss: 9.9960\n",
      "\n",
      "Epoch 00123: val_loss improved from 10.00863 to 9.99600, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 124/300\n",
      "600/600 [==============================] - 59s 99ms/step - loss: 9.3600 - val_loss: 10.0059\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 9.99600\n",
      "Epoch 125/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.3428 - val_loss: 9.9887\n",
      "\n",
      "Epoch 00125: val_loss improved from 9.99600 to 9.98873, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 126/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.3358 - val_loss: 10.0049\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 9.98873\n",
      "Epoch 127/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.3149 - val_loss: 9.9798\n",
      "\n",
      "Epoch 00127: val_loss improved from 9.98873 to 9.97976, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 128/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.2915 - val_loss: 9.9620\n",
      "\n",
      "Epoch 00128: val_loss improved from 9.97976 to 9.96198, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 129/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.2858 - val_loss: 9.9397\n",
      "\n",
      "Epoch 00129: val_loss improved from 9.96198 to 9.93972, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 130/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.3321 - val_loss: 9.9170\n",
      "\n",
      "Epoch 00130: val_loss improved from 9.93972 to 9.91703, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 131/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.2977 - val_loss: 9.9281\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 9.91703\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 54s 90ms/step - loss: 9.2526 - val_loss: 9.9503\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 9.91703\n",
      "Epoch 133/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.2509 - val_loss: 9.8992\n",
      "\n",
      "Epoch 00133: val_loss improved from 9.91703 to 9.89916, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 134/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.2430 - val_loss: 9.8887\n",
      "\n",
      "Epoch 00134: val_loss improved from 9.89916 to 9.88869, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 135/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.2545 - val_loss: 9.9289\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 9.88869\n",
      "Epoch 136/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.2215 - val_loss: 9.8745\n",
      "\n",
      "Epoch 00136: val_loss improved from 9.88869 to 9.87450, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 137/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 9.2062 - val_loss: 9.9144\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 9.87450\n",
      "Epoch 138/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.2189 - val_loss: 9.8854\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 9.87450\n",
      "Epoch 139/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.1946 - val_loss: 9.8908\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 9.87450\n",
      "Epoch 140/300\n",
      "600/600 [==============================] - 52s 86ms/step - loss: 9.2190 - val_loss: 9.8789\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 9.87450\n",
      "Epoch 141/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 9.2027 - val_loss: 9.8427\n",
      "\n",
      "Epoch 00141: val_loss improved from 9.87450 to 9.84274, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 142/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.1644 - val_loss: 9.8179\n",
      "\n",
      "Epoch 00142: val_loss improved from 9.84274 to 9.81787, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 143/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.1772 - val_loss: 9.8634\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 9.81787\n",
      "Epoch 144/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.1692 - val_loss: 9.8095\n",
      "\n",
      "Epoch 00144: val_loss improved from 9.81787 to 9.80954, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 145/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 9.1557 - val_loss: 9.8603\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 9.80954\n",
      "Epoch 146/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.1521 - val_loss: 9.7994\n",
      "\n",
      "Epoch 00146: val_loss improved from 9.80954 to 9.79939, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 147/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 9.1599 - val_loss: 9.7932\n",
      "\n",
      "Epoch 00147: val_loss improved from 9.79939 to 9.79317, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 148/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.1322 - val_loss: 9.7705\n",
      "\n",
      "Epoch 00148: val_loss improved from 9.79317 to 9.77053, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 149/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.1364 - val_loss: 9.8276\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 9.77053\n",
      "Epoch 150/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.1395 - val_loss: 9.7743\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 9.77053\n",
      "Epoch 151/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.1190 - val_loss: 9.7851\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 9.77053\n",
      "Epoch 152/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 9.1008 - val_loss: 9.7763\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 9.77053\n",
      "Epoch 153/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.0968 - val_loss: 9.7297\n",
      "\n",
      "Epoch 00153: val_loss improved from 9.77053 to 9.72973, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 154/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.1290 - val_loss: 9.8160\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 9.72973\n",
      "Epoch 155/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 9.0769 - val_loss: 9.7147\n",
      "\n",
      "Epoch 00155: val_loss improved from 9.72973 to 9.71474, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 156/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 9.0727 - val_loss: 9.7559\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 9.71474\n",
      "Epoch 157/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.0535 - val_loss: 9.6973\n",
      "\n",
      "Epoch 00157: val_loss improved from 9.71474 to 9.69731, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 158/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.0807 - val_loss: 9.7048\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 9.69731\n",
      "Epoch 159/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.0615 - val_loss: 9.7049\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 9.69731\n",
      "Epoch 160/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 9.0443 - val_loss: 9.7937\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 9.69731\n",
      "Epoch 161/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 9.0569 - val_loss: 9.6737\n",
      "\n",
      "Epoch 00161: val_loss improved from 9.69731 to 9.67368, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 162/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.0303 - val_loss: 9.7363\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 9.67368\n",
      "Epoch 163/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 9.0581 - val_loss: 9.7324\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 9.67368\n",
      "Epoch 164/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 9.0364 - val_loss: 9.6927\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 9.67368\n",
      "Epoch 165/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.0189 - val_loss: 9.6922\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 9.67368\n",
      "Epoch 166/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.0285 - val_loss: 9.7816\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 9.67368\n",
      "Epoch 167/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.0163 - val_loss: 9.6589\n",
      "\n",
      "Epoch 00167: val_loss improved from 9.67368 to 9.65886, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 168/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 9.0261 - val_loss: 9.8247\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 9.65886\n",
      "Epoch 169/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 9.0095 - val_loss: 9.6190\n",
      "\n",
      "Epoch 00169: val_loss improved from 9.65886 to 9.61904, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 170/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.0070 - val_loss: 9.7058\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 9.61904\n",
      "Epoch 171/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9794 - val_loss: 9.6090\n",
      "\n",
      "Epoch 00171: val_loss improved from 9.61904 to 9.60897, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 172/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 9.0034 - val_loss: 9.7569\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 9.60897\n",
      "Epoch 173/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.9678 - val_loss: 9.5935\n",
      "\n",
      "Epoch 00173: val_loss improved from 9.60897 to 9.59350, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 174/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9692 - val_loss: 9.6367\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 9.59350\n",
      "Epoch 175/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.9582 - val_loss: 9.5973\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 9.59350\n",
      "Epoch 176/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9625 - val_loss: 9.6031\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 9.59350\n",
      "Epoch 177/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.9506 - val_loss: 9.5747\n",
      "\n",
      "Epoch 00177: val_loss improved from 9.59350 to 9.57475, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 178/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.9483 - val_loss: 9.6049\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 9.57475\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 55s 91ms/step - loss: 8.9740 - val_loss: 9.5805\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 9.57475\n",
      "Epoch 180/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9416 - val_loss: 9.5636\n",
      "\n",
      "Epoch 00180: val_loss improved from 9.57475 to 9.56356, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 181/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9570 - val_loss: 9.6700\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 9.56356\n",
      "Epoch 182/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.9189 - val_loss: 9.5489\n",
      "\n",
      "Epoch 00182: val_loss improved from 9.56356 to 9.54890, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 183/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.9427 - val_loss: 9.5576\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 9.54890\n",
      "Epoch 184/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9327 - val_loss: 9.5444\n",
      "\n",
      "Epoch 00184: val_loss improved from 9.54890 to 9.54437, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 185/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.9098 - val_loss: 9.6141\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 9.54437\n",
      "Epoch 186/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.9298 - val_loss: 9.5557\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 9.54437\n",
      "Epoch 187/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9203 - val_loss: 9.5300\n",
      "\n",
      "Epoch 00187: val_loss improved from 9.54437 to 9.52999, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 188/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.8937 - val_loss: 9.5249\n",
      "\n",
      "Epoch 00188: val_loss improved from 9.52999 to 9.52493, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 189/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.9023 - val_loss: 9.5773\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 9.52493\n",
      "Epoch 190/300\n",
      "600/600 [==============================] - 58s 96ms/step - loss: 8.9227 - val_loss: 9.5274\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 9.52493\n",
      "Epoch 191/300\n",
      "600/600 [==============================] - 57s 95ms/step - loss: 8.9017 - val_loss: 9.4857\n",
      "\n",
      "Epoch 00191: val_loss improved from 9.52493 to 9.48571, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 192/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.8832 - val_loss: 9.5669\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 9.48571\n",
      "Epoch 193/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8922 - val_loss: 9.5621\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 9.48571\n",
      "Epoch 194/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.8679 - val_loss: 9.5772\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 9.48571\n",
      "Epoch 195/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.8870 - val_loss: 9.5830\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 9.48571\n",
      "Epoch 196/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.8754 - val_loss: 9.4623\n",
      "\n",
      "Epoch 00196: val_loss improved from 9.48571 to 9.46231, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 197/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.9093 - val_loss: 9.5289\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 9.46231\n",
      "Epoch 198/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8720 - val_loss: 9.5607\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 9.46231\n",
      "Epoch 199/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.8805 - val_loss: 9.6863\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 9.46231\n",
      "Epoch 200/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8820 - val_loss: 9.5558\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 9.46231\n",
      "Epoch 201/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8752 - val_loss: 9.6351\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 9.46231\n",
      "Epoch 202/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.8694 - val_loss: 9.5449\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 9.46231\n",
      "Epoch 203/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.8287 - val_loss: 9.5379\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 9.46231\n",
      "Epoch 204/300\n",
      "600/600 [==============================] - 58s 96ms/step - loss: 8.8287 - val_loss: 9.4616\n",
      "\n",
      "Epoch 00204: val_loss improved from 9.46231 to 9.46162, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 205/300\n",
      "600/600 [==============================] - 63s 105ms/step - loss: 8.8487 - val_loss: 9.4586\n",
      "\n",
      "Epoch 00205: val_loss improved from 9.46162 to 9.45858, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 206/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.8681 - val_loss: 9.5238\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 9.45858\n",
      "Epoch 207/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8202 - val_loss: 9.5216\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 9.45858\n",
      "Epoch 208/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8326 - val_loss: 9.5180\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 9.45858\n",
      "Epoch 209/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8198 - val_loss: 9.4196\n",
      "\n",
      "Epoch 00209: val_loss improved from 9.45858 to 9.41959, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 210/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.8058 - val_loss: 9.4623\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 9.41959\n",
      "Epoch 211/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.8422 - val_loss: 9.5300\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 9.41959\n",
      "Epoch 212/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8066 - val_loss: 9.5597\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 9.41959\n",
      "Epoch 213/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8047 - val_loss: 9.4233\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 9.41959\n",
      "Epoch 214/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.8087 - val_loss: 9.4429\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 9.41959\n",
      "Epoch 215/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.7898 - val_loss: 9.3881\n",
      "\n",
      "Epoch 00215: val_loss improved from 9.41959 to 9.38809, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 216/300\n",
      "600/600 [==============================] - 57s 95ms/step - loss: 8.8047 - val_loss: 9.4596\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 9.38809\n",
      "Epoch 217/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.7896 - val_loss: 9.4106\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 9.38809\n",
      "Epoch 218/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7857 - val_loss: 9.5071\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 9.38809\n",
      "Epoch 219/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.8089 - val_loss: 9.4139\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 9.38809\n",
      "Epoch 220/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7722 - val_loss: 9.3844\n",
      "\n",
      "Epoch 00220: val_loss improved from 9.38809 to 9.38445, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 221/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.7743 - val_loss: 9.4941\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 9.38445\n",
      "Epoch 222/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7871 - val_loss: 9.4537\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 9.38445\n",
      "Epoch 223/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.7528 - val_loss: 9.4554\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 9.38445\n",
      "Epoch 224/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.8013 - val_loss: 9.3833\n",
      "\n",
      "Epoch 00224: val_loss improved from 9.38445 to 9.38335, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 225/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7698 - val_loss: 9.3788\n",
      "\n",
      "Epoch 00225: val_loss improved from 9.38335 to 9.37881, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 226/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7550 - val_loss: 9.4518\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 9.37881\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 54s 89ms/step - loss: 8.7700 - val_loss: 9.4333\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 9.37881\n",
      "Epoch 228/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.7507 - val_loss: 9.4576\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 9.37881\n",
      "Epoch 229/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.7651 - val_loss: 9.4076\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 9.37881\n",
      "Epoch 230/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.7336 - val_loss: 9.3326\n",
      "\n",
      "Epoch 00230: val_loss improved from 9.37881 to 9.33264, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 231/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7793 - val_loss: 9.3704\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 9.33264\n",
      "Epoch 232/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.7464 - val_loss: 9.4749\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 9.33264\n",
      "Epoch 233/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7695 - val_loss: 9.3559\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 9.33264\n",
      "Epoch 234/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7761 - val_loss: 9.4255\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 9.33264\n",
      "Epoch 235/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7473 - val_loss: 9.4435\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 9.33264\n",
      "Epoch 236/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.7070 - val_loss: 9.3812\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 9.33264\n",
      "Epoch 237/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.7122 - val_loss: 9.4052\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 9.33264\n",
      "Epoch 238/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7473 - val_loss: 9.4025\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 9.33264\n",
      "Epoch 239/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.7193 - val_loss: 9.3883\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 9.33264\n",
      "Epoch 240/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7017 - val_loss: 9.6001\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 9.33264\n",
      "Epoch 241/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7337 - val_loss: 9.3231\n",
      "\n",
      "Epoch 00241: val_loss improved from 9.33264 to 9.32306, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 242/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.7014 - val_loss: 9.3537\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 9.32306\n",
      "Epoch 243/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6837 - val_loss: 9.5025\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 9.32306\n",
      "Epoch 244/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.7279 - val_loss: 9.4415\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 9.32306\n",
      "Epoch 245/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.6872 - val_loss: 9.4133\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 9.32306\n",
      "Epoch 246/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.7048 - val_loss: 9.2786\n",
      "\n",
      "Epoch 00246: val_loss improved from 9.32306 to 9.27859, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 247/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.6865 - val_loss: 9.4275\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 9.27859\n",
      "Epoch 248/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.6797 - val_loss: 9.3311\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 9.27859\n",
      "Epoch 249/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6628 - val_loss: 9.3737\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 9.27859\n",
      "Epoch 250/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6856 - val_loss: 9.3247\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 9.27859\n",
      "Epoch 251/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6556 - val_loss: 9.2776\n",
      "\n",
      "Epoch 00251: val_loss improved from 9.27859 to 9.27761, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 252/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.6709 - val_loss: 9.3468\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 9.27761\n",
      "Epoch 253/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.6397 - val_loss: 9.2580\n",
      "\n",
      "Epoch 00253: val_loss improved from 9.27761 to 9.25805, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 254/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.7409 - val_loss: 9.2755\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 9.25805\n",
      "Epoch 255/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6758 - val_loss: 9.2976\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 9.25805\n",
      "Epoch 256/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.6597 - val_loss: 9.3375\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 9.25805\n",
      "Epoch 257/300\n",
      "600/600 [==============================] - 58s 97ms/step - loss: 8.6412 - val_loss: 9.2447\n",
      "\n",
      "Epoch 00257: val_loss improved from 9.25805 to 9.24468, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 258/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.6996 - val_loss: 9.2514\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 9.24468\n",
      "Epoch 259/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.6538 - val_loss: 9.2654\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 9.24468\n",
      "Epoch 260/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6489 - val_loss: 9.3047\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 9.24468\n",
      "Epoch 261/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.6444 - val_loss: 9.2517\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 9.24468\n",
      "Epoch 262/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6312 - val_loss: 9.3115\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 9.24468\n",
      "Epoch 263/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.6421 - val_loss: 9.3874\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 9.24468\n",
      "Epoch 264/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6286 - val_loss: 9.3015\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 9.24468\n",
      "Epoch 265/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.6259 - val_loss: 9.2486\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 9.24468\n",
      "Epoch 266/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.6337 - val_loss: 9.2692\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 9.24468\n",
      "Epoch 267/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.6584 - val_loss: 9.2784\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 9.24468\n",
      "Epoch 268/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.6201 - val_loss: 9.3365\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 9.24468\n",
      "Epoch 269/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.6136 - val_loss: 9.2683\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 9.24468\n",
      "Epoch 270/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.6042 - val_loss: 9.2805\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 9.24468\n",
      "Epoch 271/300\n",
      "600/600 [==============================] - 66s 111ms/step - loss: 8.6078 - val_loss: 9.2818\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 9.24468\n",
      "Epoch 272/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.6279 - val_loss: 9.2202\n",
      "\n",
      "Epoch 00272: val_loss improved from 9.24468 to 9.22023, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 273/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.6317 - val_loss: 9.3477\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 9.22023\n",
      "Epoch 274/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.5870 - val_loss: 9.1953\n",
      "\n",
      "Epoch 00274: val_loss improved from 9.22023 to 9.19530, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 275/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.6110 - val_loss: 9.3151\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 9.19530\n",
      "Epoch 276/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.6157 - val_loss: 9.2212\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 9.19530\n",
      "Epoch 277/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 54s 90ms/step - loss: 8.5947 - val_loss: 9.3199\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 9.19530\n",
      "Epoch 278/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6038 - val_loss: 9.2565\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 9.19530\n",
      "Epoch 279/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.5815 - val_loss: 9.1859\n",
      "\n",
      "Epoch 00279: val_loss improved from 9.19530 to 9.18587, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 280/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6019 - val_loss: 9.1960\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 9.18587\n",
      "Epoch 281/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.5916 - val_loss: 9.2522\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 9.18587\n",
      "Epoch 282/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.5775 - val_loss: 9.2523\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 9.18587\n",
      "Epoch 283/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.5752 - val_loss: 9.1994\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 9.18587\n",
      "Epoch 284/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.5727 - val_loss: 9.2718\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 9.18587\n",
      "Epoch 285/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.5915 - val_loss: 9.2297\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 9.18587\n",
      "Epoch 286/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.6027 - val_loss: 9.3096\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 9.18587\n",
      "Epoch 287/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.6195 - val_loss: 9.4470\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 9.18587\n",
      "Epoch 288/300\n",
      "600/600 [==============================] - 54s 89ms/step - loss: 8.6210 - val_loss: 9.1610\n",
      "\n",
      "Epoch 00288: val_loss improved from 9.18587 to 9.16100, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 289/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.5731 - val_loss: 9.1538\n",
      "\n",
      "Epoch 00289: val_loss improved from 9.16100 to 9.15376, saving model to vgg16_eds_10312021_lr_7.h5\n",
      "Epoch 290/300\n",
      "600/600 [==============================] - 53s 89ms/step - loss: 8.5647 - val_loss: 9.2786\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 9.15376\n",
      "Epoch 291/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.5581 - val_loss: 9.3397\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 9.15376\n",
      "Epoch 292/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.5922 - val_loss: 9.3207\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 9.15376\n",
      "Epoch 293/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.5926 - val_loss: 9.2271\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 9.15376\n",
      "Epoch 294/300\n",
      "600/600 [==============================] - 54s 91ms/step - loss: 8.5400 - val_loss: 9.2204\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 9.15376\n",
      "Epoch 295/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.5537 - val_loss: 9.2344\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 9.15376\n",
      "Epoch 296/300\n",
      "600/600 [==============================] - 55s 91ms/step - loss: 8.5323 - val_loss: 9.1544\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 9.15376\n",
      "Epoch 297/300\n",
      "600/600 [==============================] - 53s 88ms/step - loss: 8.5448 - val_loss: 9.2677\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 9.15376\n",
      "Epoch 298/300\n",
      "600/600 [==============================] - 55s 92ms/step - loss: 8.5295 - val_loss: 9.2443\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 9.15376\n",
      "Epoch 299/300\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 8.5274 - val_loss: 9.2292\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 9.15376\n",
      "Epoch 300/300\n",
      "600/600 [==============================] - 54s 90ms/step - loss: 8.5288 - val_loss: 9.1709\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 9.15376\n"
     ]
    }
   ],
   "source": [
    "eds_history_10312021_lr_7 = model_lr_7.fit(X_EDS_train,Y_EDS_train,validation_data=(X_EDS_test,Y_EDS_test),epochs=300, batch_size=16,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_7.save(\"vgg16_modelfile_10312021_10sample_epo200_batch16_2.h5\")\n",
    "model_lr_7.save_weights('vgg16_weightsfile_10312021_10sample_epo200_batch16_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds_history_10312021_df = pd.DataFrame(eds_history_10312021_lr_7.history) \n",
    "eds_history_10312021_df.to_excel('saved_training_histroy_10312021.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_10312021 = model_lr_7.predict(X_EDS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, \"Ground truth Young's modulus \\nfrom indenation experiments, GPa\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "ax.scatter(Y_EDS_test,pred_10312021)\n",
    "ax.set_xlim([10,30])\n",
    "ax.set_ylim([10,30])\n",
    "ax.set_ylabel(\"Predicted Young's modulus, GPa\")\n",
    "ax.set_xlabel(\"Ground truth Young's modulus \\nfrom indenation experiments, GPa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
